---
title: "Predicting Sports Performance with Machine Learning"
author: "hugominze"
date: "2022-14-09"
output:
  html_document:
    toc: true
    toc_depth: 3
---
e
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this project is to predict the manner in which people did a specific 
exercise in sports. To get there, we analyze data of six participants' 
accelerometers. As a result, we will get 20 performance predictions.

You can read about the data more on this website:
[http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).
or in the separate README file.

## I. Data Preparation

In the first chapter, we will first set the working directory, load the necessary packages, 
set the seed and download and read in the data. We will also take a first glimpse at the data.

### 1. Set working directory

### 2. Load packages

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(randomForest)
```

### 3. set seed
```{r}
set.seed(123)
```

### 4. download data:
```{r import-data, cache=TRUE}
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if (!file.exists("data")) {
        dir.create("data")
}
download.file(url, "./data/training.csv", method="curl")

dataDownloaded <- date()
dataDownloaded

list.files("./data")

url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists("data")) {
        dir.create("data")
}
download.file(url1, "./data/testing.csv", method="curl")

dataDownloaded <- date()
dataDownloaded

list.files("./data")
```

```{r}
data1 <- read.csv("./data/training.csv", na.strings=c("NA", "NAN", "#DIV/0!", "")) #create tibble:
data_training <- as_tibble(data1)

data2 <- read.csv("./data/testing.csv", na.strings=c("NA", "NAN", "#DIV/0!", "")) #create tibble:
data_testing <- as_tibble(data2)

remove(data1)
remove(data2)
```

### 5. first glimpse at data

taking a look at the dimensions of both datasets:
```{r}
dim(data_training)
dim(data_testing)

```
We can see that classe is missing in the data_testing. Instead, there is a variable 
called problem_id.

taking a look at first few rows of data_training and data_testing:
```{r}
head(data_training)
head(data_testing)
```

taking a look at summary of data_training:
```{r}
str(data_training)
```

## II. Preprocessing

In the second chapter, we do some preprocessing. We convert the classe variable 
to a factor variable, exclude columns with a lot of NAs, delete a couple of 
irrelevant columns and convert values into numeric ones.

### 1. converting "classe" to factor
```{r}
data_training$classe <- factor(data_training$classe)

```

```{r}
qplot(data_training$classe)
```

Looking at the plot we can see that it more or less equally distributed, but 
category A has the most weight.

### 2. exclude columns with NA values from training set, because random forests cannot handle too many NA values
```{r}
data_training <- data_training[,colSums(is.na(data_training))<13000]
colSums(is.na(data_training))
```


### 3. deleting first couple of columns (ids, also time stamps and if window was open or not)
```{r}
data_training <- data_training %>% select(-(1:7))
```

### 4. convert into numeric

we can see that there is the factor variable (classe) and integer and numeric variables. 
```{r}
table(sapply(data_training,class))
```
we transform the integer into numeric columns.
```{r}
data_training <- data_training %>% mutate_if(is.integer, as.numeric)
```


## III. Developing a machine learning model

In the third chapter, we first split the `data_training` data into a training 
and validation set. Then, we preprocess the training data, so we can build our 
model: exclude rows with too many NA values, delete columns that are unnecessary. 
Next, we train different models on the training set. We then use our validation 
set to check which model has the best accuracy. Next, we use the best model to 
predict the "classe" variable on the testing set.

### 1. create validation and training dataset from `data_training`
```{r}
inTrain <- createDataPartition(y=data_training$classe,
                              p=0.70, list=FALSE)
training <- data_training[inTrain,]
validation <- data_training[-inTrain,]
dim(training)

remove(data_training)
```


### 2. set.seed and train different models (random forests, rpart)
```{r, cache=TRUE}
set.seed(1234)

# rpart
modFitrpart <- train(classe ~ .,method="rpart",data=training)
modFitrpart
modFitrpart$finalModel

rf <- randomForest(classe~., data=training)
rf

```

### 3. use `validation` set to look at different accuracy:

use validation to test `rf` and `ModFitrpart` model fit:
```{r include=FALSE}
set.seed(123456)
#rf
predrf <- predict(rf, validation)
confusionMatrix(factor(predrf), factor(validation$classe))$overall[1]

#rpart
predrpart <- predict(modFitrpart, validation)
confusionMatrix(factor(predrpart), factor(validation$classe))$overall[1]

```
We can see that the random forests algorithm performed best with an accuracy of 
more than 99 percent on the validation set.

## IV. Results

Lastly, we take the best model (random forests rf) and predict the "classe" values for the `data_testing` data:

```{r}
pred_test <- predict(rf, data_testing)
pred_test
```
As a result, we get 20 predictions for the data_testing set.
